# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import requests
import json
import os
import sys
import datetime
import pandas as pd
import matplotlib.pyplot as plt
from google.cloud import language_v1
from transformers import pipeline
import csv
from sentistrength import PySentiStr
import nltk
from nltk.corpus import wordnet
import feedparser
import time
import re
from random import randint
import seaborn as sns  # ì¶”ê°€

# Google API ì¸ì¦ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸ í•„ìš”)
GOOGLE_APPLICATION_CREDENTIALS = "C:\\Users\\user\\Desktop\\ì—°êµ¬\\êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬\\ì½”ë”©\\comparative-sentiment-analysis-c0b363950560.json"
HUGGINGFACE_API_KEY = "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"

# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.download("wordnet", quiet=True)
except:
    print("NLTK wordnet ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ì˜¤í”„ë¼ì¸ì´ê±°ë‚˜ NLTK ì„¤ì¹˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")

# ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ì˜
NEWS_SOURCES = {
    "USA": {
        "The New York Times": {"bias": "liberal"},
        "CNN": {"bias": "liberal"},
        "Fox News": {"bias": "conservative"}
    },
    "UK": {
        "The Guardian": {"bias": "liberal"},
        "The Telegraph": {"bias": "conservative"},
        "BBC News": {"bias": "neutral"}
    }
}

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì§€ì •
RESULTS_DIR = "C:\\Users\\user\\Desktop\\ì—°êµ¬\\êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬\\ì½”ë”©\\ê²°ê³¼"

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
DEBUG_MODE = True

def debug_print(message, data=None, important=False):
    """ë””ë²„ê·¸ ë©”ì‹œì§€ ì¶œë ¥ í•¨ìˆ˜"""
    if DEBUG_MODE:
        if important:
            print("\n" + "="*50)
            print(f"ğŸ” {message}")
            print("="*50)
        else:
            print(f"ğŸ”¹ {message}")
        
        if data is not None:
            if isinstance(data, str) and len(data) > 300:
                print(f"{data[:300]}... (ìƒëµë¨)")
            else:
                print(data)

# ì˜¤ë¥˜ ë©”ì‹œì§€ ì„¤ì •
def setup_error_handling():
    """ì˜¤ë¥˜ ì²˜ë¦¬ ì„¤ì •"""
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)
    print("ì•Œë¦¼: êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµë¥¼ ìœ„í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

# WordNetì„ í™œìš©í•œ ë™ì˜ì–´ í™•ì¥
def get_synonyms(keyword):
    """ë‹¨ì–´ì˜ ë™ì˜ì–´ ëª©ë¡ ë°˜í™˜"""
    synonyms = set()
    try:
        for syn in wordnet.synsets(keyword):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name().replace('_', ' '))
    except Exception as e:
        debug_print(f"ë™ì˜ì–´ ì°¾ê¸° ì˜¤ë¥˜: {e}")
    return list(synonyms)

# ì†ŒìŠ¤ ë„ë©”ì¸ ê°€ì ¸ì˜¤ê¸°
def get_source_domain(source):
    """ë‰´ìŠ¤ ì†ŒìŠ¤ì˜ ë„ë©”ì¸ ë°˜í™˜"""
    source_domains = {
        'CNN': 'cnn.com',
        'Fox News': 'foxnews.com',
        'The Guardian': 'theguardian.com',
        'The New York Times': 'nytimes.com',
        'BBC News': 'bbc.com,bbc.co.uk',
        'The Telegraph': 'telegraph.co.uk',
        'Reuters': 'reuters.com',
        'CNBC': 'cnbc.com',
        'Bloomberg': 'bloomberg.com'
    }
    
    return source_domains.get(source, '')

# ì†ŒìŠ¤ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
def is_same_source(found_source, target_source):
    """ë°œê²¬ëœ ì†ŒìŠ¤ê°€ ëª©í‘œ ì†ŒìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸"""
    found_lower = found_source.lower()
    target_lower = target_source.lower()
    
    # ì •í™•íˆ ì¼ì¹˜
    if found_lower == target_lower:
        return True
    
    # ë¶€ë¶„ ë¬¸ìì—´ í™•ì¸
    if target_lower in found_lower or found_lower in target_lower:
        return True
    
    # CNN, NYT ë“± ì•½ì–´ í™•ì¸
    abbreviations = {
        'cnn': ['cnn'],
        'fox news': ['fox', 'fox news'],
        'the guardian': ['guardian', 'the guardian'],
        'the new york times': ['nyt', 'ny times', 'new york times'],
        'bbc news': ['bbc'],
        'the telegraph': ['telegraph'],
        'reuters': ['reuters']
    }
    
    target_abbr = abbreviations.get(target_lower, [])
    for abbr in target_abbr:
        if abbr in found_lower:
            return True
    
    return False

# Google News RSSì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰
def get_google_news_rss(keyword, source=None):
    """Google News RSSì—ì„œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
    # ì†ŒìŠ¤ ì§€ì •ì´ ìˆìœ¼ë©´ ì†ŒìŠ¤ í•„í„°ë§ ì¶”ê°€
    query = keyword
    if source:
        query = f"{keyword} site:{get_source_domain(source)}"
    
    # URL ì¸ì½”ë”©
    query = requests.utils.quote(query)
    url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
    debug_print(f"Google News RSS ìš”ì²­ URL: {url}")
    
    try:
        feed = feedparser.parse(url)
        
        articles = []
        for entry in feed.entries[:15]:  # ì¡°ê¸ˆ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§í•  ì—¬ì§€ í™•ë³´
            # ì†ŒìŠ¤ ì¶”ì¶œ
            title_parts = entry.title.split(" - ")
            entry_source = title_parts[-1].strip() if len(title_parts) > 1 else "Unknown"
            
            # ì†ŒìŠ¤ í™•ì¸ - ë” ì—„ê²©í•œ ê²€ì‚¬
            if source and not is_same_source(entry_source, source):
                debug_print(f"ì†ŒìŠ¤ ë¶ˆì¼ì¹˜: '{entry_source}' â‰  '{source}' - ê±´ë„ˆëœ€")
                continue
                
            article = {
                'title': title_parts[0].strip(),
                'content': entry.description if hasattr(entry, 'description') else title_parts[0].strip(),
                'source': entry_source,
                'url': entry.link,
                'published_at': entry.published if hasattr(entry, 'published') else datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            articles.append(article)
        
        debug_print(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼: {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
        return articles
        
    except Exception as e:
        debug_print(f"Google News RSS ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

# í‚¤ì›Œë“œ í™•ì¥ + Google News ê²€ìƒ‰ ê²°í•©
def search_expanded_news(keyword, source=None):
    """í‚¤ì›Œë“œ í™•ì¥ í›„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    debug_print(f"'{keyword}' í™•ì¥ ê²€ìƒ‰ ì‹œì‘ (ì†ŒìŠ¤: {source})", important=True)
    
    # Step 1: ì›ë˜ í‚¤ì›Œë“œë¡œ Google News ê²€ìƒ‰
    initial_news = get_google_news_rss(keyword, source)
    
    if len(initial_news) >= 5:  # ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        debug_print(f"ì›ë˜ í‚¤ì›Œë“œë¡œ ì¶©ë¶„í•œ ê²°ê³¼({len(initial_news)}ê°œ)ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return initial_news[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
    
    # Step 2: í‚¤ì›Œë“œ í™•ì¥ì´ í•„ìš”í•œ ê²½ìš°
    debug_print(f"ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ í™•ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    expanded_keywords = [keyword]  # ì›ë˜ í‚¤ì›Œë“œ í¬í•¨
    
    # ë‹¨ì¼ ë‹¨ì–´ì¸ ê²½ìš° WordNet ë™ì˜ì–´ ì¶”ê°€
    if ' ' not in keyword and len(keyword) > 3:
        synonyms = get_synonyms(keyword)[:3]  # ìƒìœ„ 3ê°œ ë™ì˜ì–´ë§Œ ì‚¬ìš©
        expanded_keywords.extend(synonyms)
    
    # ì¤‘ë³µ ì œê±°
    expanded_keywords = list(set(expanded_keywords))
    debug_print(f"í™•ì¥ëœ í‚¤ì›Œë“œ: {expanded_keywords}")
    
    # Step 3: í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
    all_news = initial_news.copy()  # ì´ˆê¸° ê²°ê³¼ í¬í•¨
    
    for k in expanded_keywords:
        if k == keyword:  # ì›ë˜ í‚¤ì›Œë“œëŠ” ì´ë¯¸ ê²€ìƒ‰í–ˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœ€
            continue
            
        debug_print(f"í™•ì¥ í‚¤ì›Œë“œ '{k}'ë¡œ ê²€ìƒ‰ ì¤‘...")
        additional_news = get_google_news_rss(k, source)
        
        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
        for article in additional_news:
            # URLë¡œ ì¤‘ë³µ í™•ì¸
            if not any(existing['url'] == article['url'] for existing in all_news):
                all_news.append(article)
    
    debug_print(f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(all_news)}ê°œ ê¸°ì‚¬")
    return all_news[:5]  # ìµœëŒ€ 5ê°œë§Œ ë°˜í™˜

# ê°ì • ë¶„ì„ í•¨ìˆ˜ë“¤

def get_vader_sentiment(text):
    """VADERë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        sid = SentimentIntensityAnalyzer()
        sentiment_scores = sid.polarity_scores(text)
        # compound ì ìˆ˜ëŠ” -1ì—ì„œ 1 ì‚¬ì´ì˜ ê°’
        compound_score = sentiment_scores['compound']
        print(f"VADER ì›ë³¸ ì ìˆ˜: {compound_score}")
        return compound_score
    except Exception as e:
        print(f"VADER ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0

def get_sentistrength_sentiment(text):
    """SentiStrengthë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        # SentiStrength ì´ˆê¸°í™”
        senti = PySentiStr()
        
        # SentiStrength.jar íŒŒì¼ ê²½ë¡œ ì„¤ì •
        senti.setSentiStrengthPath("C:\\Users\\user\\Desktop\\ì—°êµ¬\\êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬\\ì½”ë”©\\SentiStrength.jar")
        
        # ì–¸ì–´ í´ë” ê²½ë¡œ ì„¤ì •
        senti.setSentiStrengthLanguageFolderPath("C:\\Users\\user\\Desktop\\ì—°êµ¬\\êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬\\ì½”ë”©\\SentiStrengthDataEnglishOctober2019")
        
        # dual ì ìˆ˜ë¡œ ë¶„ì„ (ê¸ì • ë° ë¶€ì • ì ìˆ˜ ê°ê° ë°˜í™˜)
        result = senti.getSentiment(text, score='dual')
        
        # ê²°ê³¼ëŠ” íŠœí”Œ í˜•íƒœ: (positive_score, negative_score)
        positive_score, negative_score = result[0]
        
        print(f"SentiStrength ì›ë³¸ ì ìˆ˜: ê¸ì •={positive_score}, ë¶€ì •={negative_score}")
        return positive_score, negative_score
    except Exception as e:
        print(f"SentiStrength ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í”¼ë“œë°± ì¶”ê°€
        print(f"ê²½ë¡œ í™•ì¸: JAR={senti.SentiStrengthPath}, ë°ì´í„°={senti.SentiStrengthLanguageFolderPath}")
        print("Javaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: 'java -version' ëª…ë ¹ì–´ë¡œ í™•ì¸ ê°€ëŠ¥")
        return 1, -1  # ì¤‘ë¦½ ê°’ ë°˜í™˜

def get_google_sentiment(text):
    """Google Natural Language APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        # Google Cloud ì¸ì¦ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = GOOGLE_APPLICATION_CREDENTIALS
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = language_v1.LanguageServiceClient()
        
        # ë¬¸ì„œ ê°ì²´ ìƒì„±
        document = language_v1.Document(
            content=text,
            type_=language_v1.Document.Type.PLAIN_TEXT
        )
        
        # ê°ì • ë¶„ì„ ìˆ˜í–‰
        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment
        
        # Google APIëŠ” -1.0 ~ 1.0 ë²”ìœ„ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜
        score = sentiment.score
        print(f"Google API ì›ë³¸ ì ìˆ˜: {score}")
        return score
    except Exception as e:
        print(f"Google ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ê°’ ë°˜í™˜

def get_huggingface_sentiment(text):
    try:
        # ëª¨ë¸ ëª…ì‹œì  ì§€ì •ìœ¼ë¡œ ê²½ê³  ì œê±°
        sentiment_analyzer = pipeline("sentiment-analysis", 
                                    model="distilbert-base-uncased-finetuned-sst-2-english")
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë¶„ì„)
        max_length = 512
        truncated_text = text[:max_length] if len(text) > max_length else text
        result = sentiment_analyzer(truncated_text)[0]
        
        # NEGATIVE ê²°ê³¼ì— ëŒ€í•œ ì²˜ë¦¬ ê°œì„ 
        if result['label'] == 'POSITIVE':
            score = result['score']
        else:
            score = 1 - result['score']  # ë” ì§ê´€ì ì¸ ë³€í™˜
            
        print(f"Hugging Face ì ìˆ˜: {score} (ì›ë˜ ë ˆì´ë¸”: {result['label']})")
        return score
    except Exception as e:
        print(f"Hugging Face ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0.5

# ì ìˆ˜ ì •ê·œí™” í•¨ìˆ˜ë“¤

def normalize_vader_score(score):
    """VADER ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    normalized_score = (score + 1) / 2
    return normalized_score

def normalize_sentistrength_score(positive_score, negative_score):
    """SentiStrength ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    combined_score = (positive_score + (6 + negative_score)) / 10
    return combined_score

def normalize_google_score(score):
    """Google API ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    normalized_score = (score + 1) / 2
    return normalized_score

# ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜

def calculate_final_sentiment(vader_score, sentistrength_positive, sentistrength_negative, google_score, huggingface_score):
    """ê° ë„êµ¬ì˜ ì •ê·œí™”ëœ ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚°"""
    # ê° ì ìˆ˜ ì •ê·œí™”
    normalized_vader = normalize_vader_score(vader_score)
    normalized_sentistrength = normalize_sentistrength_score(sentistrength_positive, sentistrength_negative)
    normalized_google = normalize_google_score(google_score)
    
    # ì •ê·œí™”ëœ ì ìˆ˜ ì¶œë ¥
    print(f"ì •ê·œí™”ëœ VADER ì ìˆ˜: {normalized_vader:.4f}")
    print(f"ì •ê·œí™”ëœ SentiStrength ì ìˆ˜: {normalized_sentistrength:.4f}")
    print(f"ì •ê·œí™”ëœ Google API ì ìˆ˜: {normalized_google:.4f}")
    print(f"ì •ê·œí™”ëœ Hugging Face ì ìˆ˜: {huggingface_score:.4f}")
    
    # ê·¹ë‹¨ì ì¸ ì ìˆ˜ê°€ ìˆì„ ê²½ìš° ê°€ì¤‘ì¹˜ ì¡°ì •
    # Hugging Faceê°€ 0.1 ë¯¸ë§Œì´ê±°ë‚˜ 0.9 ì´ˆê³¼ì¼ ê²½ìš° ê°€ì¤‘ì¹˜ ë‚®ì¶¤
    if huggingface_score < 0.1 or huggingface_score > 0.9:
        vader_weight = 0.3
        sentistrength_weight = 0.4
        google_weight = 0.3
        huggingface_weight = 0.0  # ê°€ì¤‘ì¹˜ ì œê±°
        print("Hugging Face ì ìˆ˜ê°€ ê·¹ë‹¨ì ì´ì–´ì„œ ê³„ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.")
    else:
        vader_weight = 0.25
        sentistrength_weight = 0.3
        google_weight = 0.25
        huggingface_weight = 0.2
        
    # ê°€ì¤‘ í‰ê·  ê³„ì‚° (Hugging Face ì œì™¸ ê°€ëŠ¥ì„± ìˆìŒ)
    if huggingface_weight > 0:
        final_score = (vader_weight * normalized_vader + 
                     sentistrength_weight * normalized_sentistrength + 
                     google_weight * normalized_google + 
                     huggingface_weight * huggingface_score)
    else:
        # í•©ê³„ê°€ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
        total_weight = vader_weight + sentistrength_weight + google_weight
        final_score = (vader_weight * normalized_vader + 
                     sentistrength_weight * normalized_sentistrength + 
                     google_weight * normalized_google) / total_weight
    
    return final_score

# ê¸°ì‚¬ ê°ì • ë¶„ì„ í•¨ìˆ˜

def analyze_article_sentiment(article, country, search_keyword=None):
    """ê¸°ì‚¬ ê°ì • ë¶„ì„ ìˆ˜í–‰"""
    title = article['title']
    content = article['content']
    source = article['source']
    text = f"{title}. {content}"  # ì œëª©ê³¼ ë‚´ìš©ì„ í•©ì³ì„œ ë¶„ì„
    
    print(f"\nì…ë ¥ ë‰´ìŠ¤ ê¸°ì‚¬: '{title}'")
    print(f"ì¶œì²˜: {source} (êµ­ê°€: {country})")
    print(f"ë‚´ìš© ì¼ë¶€: {content[:100]}...\n")
    
    # ê° ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„
    vader_score = get_vader_sentiment(text)
    sentistrength_pos, sentistrength_neg = get_sentistrength_sentiment(text)
    google_score = get_google_sentiment(text)
    huggingface_score = get_huggingface_sentiment(text)
    
    # ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚°
    final_score = calculate_final_sentiment(
        vader_score, 
        sentistrength_pos, 
        sentistrength_neg, 
        google_score, 
        huggingface_score
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n===== ê°ì • ë¶„ì„ ê²°ê³¼ =====")
    print(f"VADER ê°ì • ì ìˆ˜: {vader_score} (ì •ê·œí™”: {normalize_vader_score(vader_score):.4f})")
    print(f"SentiStrength ì ìˆ˜: ê¸ì •={sentistrength_pos}, ë¶€ì •={sentistrength_neg} (ì •ê·œí™”: {normalize_sentistrength_score(sentistrength_pos, sentistrength_neg):.4f})")
    print(f"Google API ê°ì • ì ìˆ˜: {google_score} (ì •ê·œí™”: {normalize_google_score(google_score):.4f})")
    print(f"Hugging Face ê°ì • ì ìˆ˜: {huggingface_score:.4f}")
    print(f"ìµœì¢… ê°ì • ì ìˆ˜: {final_score:.4f}")
    
    # ê°ì • í•´ì„
    if final_score > 0.65:
        sentiment = "ë§¤ìš° ê¸ì •ì "
    elif final_score > 0.55:
        sentiment = "ê¸ì •ì "
    elif final_score > 0.45:
        sentiment = "ì¤‘ë¦½ì "
    elif final_score > 0.35:
        sentiment = "ë¶€ì •ì "
    else:
        sentiment = "ë§¤ìš° ë¶€ì •ì "
    
    print(f"ê°ì • í•´ì„: {sentiment}")
    
    # ê²°ê³¼ ë°˜í™˜
    return {
        'title': title,
        'source': source,
        'country': country,
        'bias': get_source_bias(source),
        'search_keyword': search_keyword,
        'final_score': final_score,
        'vader_score': normalize_vader_score(vader_score),
        'sentistrength_score': normalize_sentistrength_score(sentistrength_pos, sentistrength_neg),
        'google_score': normalize_google_score(google_score),
        'huggingface_score': huggingface_score,
        'sentiment': sentiment,
        'url': article.get('url', '')
    }

def get_source_bias(source):
    """ì‹ ë¬¸ì‚¬ì˜ ì •ì¹˜ì  ì„±í–¥ ë°˜í™˜"""
    for country, sources in NEWS_SOURCES.items():
        if source in sources:
            return sources[source]['bias']
    return "unknown"

# êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ í•¨ìˆ˜

def compare_news_sources(keywords, articles_per_source=3):
    """ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œë³„ ê°ì • ë¶„ì„ ë¹„êµ"""
    results = []
    
    # ì›í•˜ëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤ë¡œ ì •í™•íˆ ì œí•œ
    sources = {
        'CNN': 'US',
        'Fox News': 'US',
        'The Guardian': 'UK',
        'The New York Times': 'US',
        'The Telegraph': 'UK',
        'BBC News': 'UK'
    }
    
    for keyword in keywords:
        debug_print(f"\ní‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ë¶„ì„ ì‹œì‘...", important=True)
        
        for source, country in sources.items():
            debug_print(f"{source}({country})ì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
            
            # Google News RSSë¡œ ê¸°ì‚¬ ê²€ìƒ‰
            articles = search_expanded_news(keyword, source)
            
            if not articles:
                debug_print(f"âŒ {source}ì—ì„œ '{keyword}'ì— ëŒ€í•œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ì°¾ì€ ê¸°ì‚¬ ê°œìˆ˜ ì œí•œ
            articles = articles[:articles_per_source]
            
            # ê° ê¸°ì‚¬ ë¶„ì„
            for article in articles:
                # ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                time.sleep(randint(1, 3))
                
                result = analyze_article_sentiment(article, country, search_keyword=keyword)
                
                if result:
                    results.append(result)
                    debug_print(f"âœ… ë¶„ì„ ì™„ë£Œ: {result['title']} (ì ìˆ˜: {result['final_score']})")
    
    return results

# ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥ í•¨ìˆ˜

def get_date_folder():
    """ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í´ë”ëª… ìƒì„± (ì˜ˆ: 20240601)"""
    today = datetime.datetime.now()
    return today.strftime("%Y%m%d")

def get_timestamp():
    """í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ì˜ˆ: 20240601_143045)"""
    now = datetime.datetime.now()
    return now.strftime("%Y%m%d_%H%M%S")

def get_result_folder():
    """ë‚ ì§œë³„ ê²°ê³¼ í´ë” ê²½ë¡œ ë°˜í™˜ ë° ìƒì„±"""
    date_folder = get_date_folder()
    result_path = os.path.join(RESULTS_DIR, date_folder)
    
    # ê²°ê³¼ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)
    
    # ë‚ ì§œë³„ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(result_path):
        os.makedirs(result_path)
        print(f"ìƒˆ ë‚ ì§œ í´ë” ìƒì„±: {result_path}")
    
    return result_path

def sample_article_review(df, sample_size=5):
    """Sample article review for sentiment analysis validation"""
    print("\nğŸ” Sample Article Sentiment Analysis Review")
    if len(df) < sample_size:
        sample_size = len(df)
        print(f"âš ï¸ Only {sample_size} samples available for review.")
    
    sample_articles = df.sample(sample_size)
    for idx, row in sample_articles.iterrows():
        print(f"\nArticle Title: {row['title']}")
        print(f"Source: {row['source']} (Country: {row['country']})")
        print(f"Search Keyword: {row.get('search_keyword', 'No info')}")
        print(f"Final Sentiment Score: {row['final_score']:.4f} ({row['sentiment']})")
        print(f"Tool Scores: VADER={row['vader_score']:.2f}, SentiStrength={row['sentistrength_score']:.2f}, " +
              f"Google={row['google_score']:.2f}, HuggingFace={row['huggingface_score']:.2f}")
        print(f"URL: {row.get('url', 'No info')}")
        print("=" * 50)

def visualize_sentiment_by_source(df, result_folder, timestamp):
    """Compare sentiment score distribution by news source"""
    plt.figure(figsize=(12, 6))
    ax = sns.boxplot(x='source', y='final_score', data=df)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    plt.title('Sentiment Score Distribution by News Source')
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'source_sentiment_boxplot_{timestamp}.png'))
    print(f"ğŸ“Š News source sentiment distribution graph saved.")

def compare_sentiment_tools(df, result_folder, timestamp):
    """Compare sentiment scores across different analysis tools"""
    plt.figure(figsize=(10, 6))
    tools = ['vader_score', 'sentistrength_score', 'google_score', 'huggingface_score']
    tool_labels = ['VADER', 'SentiStrength', 'Google NLP', 'HuggingFace']
    
    # Box plot
    ax = sns.boxplot(data=df[tools])
    ax.set_xticklabels(tool_labels)
    plt.title('Sentiment Score Comparison Across Analysis Tools')
    plt.ylabel('Sentiment Score (0-1)')
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'tool_comparison_{timestamp}.png'))
    
    # Correlation analysis
    plt.figure(figsize=(10, 8))
    correlation = df[tools].corr()
    sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, 
                xticklabels=tool_labels, yticklabels=tool_labels)
    plt.title('Correlation Between Sentiment Analysis Tools')
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'tool_correlation_{timestamp}.png'))
    print(f"ğŸ“Š Tool comparison graphs saved.")

def analyze_bias_influence(df, result_folder, timestamp):
    """Analyze the influence of political bias on sentiment analysis"""
    plt.figure(figsize=(12, 6))
    
    # Political bias sentiment distribution
    sns.boxplot(x='bias', y='final_score', data=df)
    plt.title('Sentiment Score Distribution by Political Bias')
    plt.xlabel('Political Bias')
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'bias_influence_{timestamp}.png'))
    
    # Tool-specific bias analysis
    tools = ['vader_score', 'sentistrength_score', 'google_score', 'huggingface_score']
    tool_labels = ['VADER', 'SentiStrength', 'Google NLP', 'HuggingFace']
    
    plt.figure(figsize=(15, 10))
    for i, tool in enumerate(tools):
        plt.subplot(2, 2, i+1)
        sns.boxplot(x='bias', y=tool, data=df)
        plt.title(f'{tool_labels[i]}')
        plt.xlabel('Political Bias')
        plt.ylabel('Sentiment Score (0-1)')
        plt.ylim(0, 1)
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'bias_by_tool_{timestamp}.png'))
    print(f"ğŸ“Š Political bias influence graphs saved.")

def evaluate_sentiment_reliability(df, result_folder, timestamp):
    """Comprehensive evaluation of sentiment analysis reliability"""
    print("\n=== Starting Sentiment Analysis Reliability Evaluation ===")
    
    # 1. Sample article review
    sample_article_review(df)
    
    # 2. News source sentiment comparison
    visualize_sentiment_by_source(df, result_folder, timestamp)
    
    # 3. Analysis tool comparison
    compare_sentiment_tools(df, result_folder, timestamp)
    
    # 4. Political bias influence analysis
    analyze_bias_influence(df, result_folder, timestamp)
    
    print("\n=== Sentiment Analysis Reliability Evaluation Complete ===")
    print(f"All evaluation graphs have been saved to {result_folder} folder.")

def visualize_results(results):
    """Visualize analysis results and evaluate reliability"""
    if not results:
        print("No results to visualize.")
        return
    
    # Get date-based result folder
    result_folder = get_result_folder()
    
    # Get current timestamp
    timestamp = get_timestamp()
    
    # Create dataframe
    df = pd.DataFrame(results)
    
    # Add analysis time to title
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    title_suffix = f" (Analysis: {current_time})"
    
    # 1. Country-based sentiment comparison
    plt.figure(figsize=(10, 6))
    country_avg = df.groupby('country')['final_score'].mean().sort_values(ascending=False)
    country_avg.plot(kind='bar', color='skyblue')
    plt.title('Average Sentiment Score by Country' + title_suffix)
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='r', linestyle='-', alpha=0.3)  # Neutral line
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'country_sentiment_{timestamp}.png'))
    
    # 2. News source sentiment comparison
    plt.figure(figsize=(12, 6))
    source_avg = df.groupby('source')['final_score'].mean().sort_values(ascending=False)
    source_avg.plot(kind='bar', color='lightgreen')
    plt.title('Average Sentiment Score by News Source' + title_suffix)
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='r', linestyle='-', alpha=0.3)  # Neutral line
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'source_sentiment_{timestamp}.png'))
    
    # 3. Political bias sentiment comparison
    plt.figure(figsize=(8, 6))
    bias_avg = df.groupby('bias')['final_score'].mean().sort_values(ascending=False)
    colors = {'liberal': 'blue', 'conservative': 'red', 'neutral': 'gray', 'unknown': 'black'}
    bias_avg.plot(kind='bar', color=[colors.get(x, 'black') for x in bias_avg.index])
    plt.title('Average Sentiment Score by Political Bias' + title_suffix)
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='r', linestyle='-', alpha=0.3)  # Neutral line
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'bias_sentiment_{timestamp}.png'))
    
    # 4. Analysis tool comparison
    plt.figure(figsize=(10, 6))
    tools = ['vader_score', 'sentistrength_score', 'google_score', 'huggingface_score']
    tool_avg = df[tools].mean()
    tool_avg.plot(kind='bar', color='orange')
    plt.title('Average Sentiment Score by Analysis Tool' + title_suffix)
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='r', linestyle='-', alpha=0.3)  # Neutral line
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'tool_sentiment_{timestamp}.png'))
    
    print(f"\nVisualization images have been saved to {result_folder} folder.")
    
    # Run reliability evaluation
    evaluate_sentiment_reliability(df, result_folder, timestamp)

def save_results_to_csv(results, filename=None):
    """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not results:
        print("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‚ ì§œë³„ ê²°ê³¼ í´ë” ê°€ì ¸ì˜¤ê¸°
    result_folder = get_result_folder()
    
    # íŒŒì¼ëª…ì— ì‹œê°„ ì¶”ê°€
    timestamp = get_timestamp()
    if filename is None:
        filename = f"news_sentiment_analysis_{timestamp}.csv"
    
    # ê²°ê³¼ì— ë¶„ì„ ì‹œê°„ ì¶”ê°€
    analysis_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    for r in results:
        r['analysis_time'] = analysis_time
    
    # ì „ì²´ íŒŒì¼ ê²½ë¡œ
    filepath = os.path.join(result_folder, filename)
    
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['search_keyword', 'title', 'source', 'country', 'bias', 'final_score', 
                     'vader_score', 'sentistrength_score', 'google_score', 'huggingface_score',
                     'sentiment', 'url', 'analysis_time']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for result in results:
            writer.writerow(result)
    
    print(f"\në¶„ì„ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì•Œë¦¼: êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµë¥¼ ìœ„í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ê²€ìƒ‰í•  í‚¤ì›Œë“œ ëª©ë¡
    keywords = [
        "W.T.O", "WTO", "World Trade Organization",
        "tariff", "tariffs",
        "trade war",
        "free trading", "free trade",
        "trade liberalization",
        "trade agreements", "trade deal",
        "global trade", "international trade",
        "import export regulations",
        "trade deficit", "trade surplus",
        "protectionism", "trade barriers"
    ]
    
    # í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œëŠ” í‚¤ì›Œë“œ ìˆ˜ ì œí•œ
    if DEBUG_MODE:
        test_keywords = ["trade"]
        debug_print(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {test_keywords} í‚¤ì›Œë“œë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", important=True)
        results = compare_news_sources(test_keywords, articles_per_source=1)
        
        if not results:
            debug_print("âŒ í…ŒìŠ¤íŠ¸ì—ì„œ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        else:
            debug_print(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼ ìƒì„±")
            
            # í…ŒìŠ¤íŠ¸ í›„ ê³„ì† ì§„í–‰í• ì§€ ë¬¼ì–´ë´„
            continue_analysis = input("í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì „ì²´ ë¶„ì„ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
            if not continue_analysis:
                return
    
    # ì „ì²´ í‚¤ì›Œë“œë¡œ ì‹¤í–‰
    results = compare_news_sources(keywords, articles_per_source=3)
    
    # ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
    if results:
        save_results_to_csv(results)
        visualize_results(results)
        debug_print(f"âœ… ì´ {len(results)}ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì €ì¥ë¨.")
    else:
        debug_print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()