# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import requests
import json
import os
import sys
import datetime
import pandas as pd
import matplotlib.pyplot as plt
from google.cloud import language_v1
from transformers import pipeline
import csv
from sentistrength import PySentiStr
import nltk
from nltk.corpus import wordnet
import feedparser
import time
import re
from random import randint
import seaborn as sns  # ì¶”ê°€
import numpy as np
from scipy.stats import zscore

# Google API ì¸ì¦ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸ í•„ìš”)
GOOGLE_APPLICATION_CREDENTIALS = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "")
HUGGINGFACE_API_KEY = os.environ.get("HUGGINGFACE_API_KEY", "")

# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.download("wordnet", quiet=True)
except:
    print("NLTK wordnet ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ì˜¤í”„ë¼ì¸ì´ê±°ë‚˜ NLTK ì„¤ì¹˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")

# ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ì˜
NEWS_SOURCES = {
    "USA": {
        "The New York Times": {"bias": "liberal"},
        "CNN": {"bias": "liberal"},
        "Fox News": {"bias": "conservative"}
    },
    "UK": {
        "The Guardian": {"bias": "liberal"},
        "The Telegraph": {"bias": "conservative"},
        "BBC News": {"bias": "neutral"}
    }
}

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì§€ì •
RESULTS_DIR = "C:\\Users\\user\\Desktop\\ì—°êµ¬\\êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬\\ì½”ë”©\\ê²°ê³¼"

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
DEBUG_MODE = True

def debug_print(message, data=None, important=False):
    """ë””ë²„ê·¸ ë©”ì‹œì§€ ì¶œë ¥ í•¨ìˆ˜"""
    if DEBUG_MODE:
        if important:
            print("\n" + "="*50)
            print(f"ğŸ” {message}")
            print("="*50)
        else:
            print(f"ğŸ”¹ {message}")
        
        if data is not None:
            if isinstance(data, str) and len(data) > 300:
                print(f"{data[:300]}... (ìƒëµë¨)")
            else:
                print(data)

# ì˜¤ë¥˜ ë©”ì‹œì§€ ì„¤ì •
def setup_error_handling():
    """ì˜¤ë¥˜ ì²˜ë¦¬ ì„¤ì •"""
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)
    print("ì•Œë¦¼: êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµë¥¼ ìœ„í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

# WordNetì„ í™œìš©í•œ ë™ì˜ì–´ í™•ì¥
def get_synonyms(keyword):
    """ë‹¨ì–´ì˜ ë™ì˜ì–´ ëª©ë¡ ë°˜í™˜"""
    synonyms = set()
    try:
        for syn in wordnet.synsets(keyword):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name().replace('_', ' '))
    except Exception as e:
        debug_print(f"ë™ì˜ì–´ ì°¾ê¸° ì˜¤ë¥˜: {e}")
    return list(synonyms)

# ì†ŒìŠ¤ ë„ë©”ì¸ ê°€ì ¸ì˜¤ê¸°
def get_source_domain(source):
    """ë‰´ìŠ¤ ì†ŒìŠ¤ì˜ ë„ë©”ì¸ ë°˜í™˜"""
    source_domains = {
        'CNN': 'cnn.com',
        'Fox News': 'foxnews.com',
        'The Guardian': 'theguardian.com',
        'The New York Times': 'nytimes.com',
        'BBC News': 'bbc.com,bbc.co.uk',
        'The Telegraph': 'telegraph.co.uk',
        'Reuters': 'reuters.com',
        'CNBC': 'cnbc.com',
        'Bloomberg': 'bloomberg.com'
    }
    
    return source_domains.get(source, '')

# ì†ŒìŠ¤ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
def is_same_source(found_source, target_source):
    """ë°œê²¬ëœ ì†ŒìŠ¤ê°€ ëª©í‘œ ì†ŒìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸"""
    found_lower = found_source.lower()
    target_lower = target_source.lower()
    
    # ì •í™•íˆ ì¼ì¹˜
    if found_lower == target_lower:
        return True
    
    # ë¶€ë¶„ ë¬¸ìì—´ í™•ì¸
    if target_lower in found_lower or found_lower in target_lower:
        return True
    
    # CNN, NYT ë“± ì•½ì–´ í™•ì¸
    abbreviations = {
        'cnn': ['cnn'],
        'fox news': ['fox', 'fox news'],
        'the guardian': ['guardian', 'the guardian'],
        'the new york times': ['nyt', 'ny times', 'new york times'],
        'bbc news': ['bbc'],
        'the telegraph': ['telegraph'],
        'reuters': ['reuters']
    }
    
    target_abbr = abbreviations.get(target_lower, [])
    for abbr in target_abbr:
        if abbr in found_lower:
            return True
    
    return False

# Google News RSSì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰
def get_google_news_rss(keyword, source=None):
    """Google News RSSì—ì„œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
    # ì†ŒìŠ¤ ì§€ì •ì´ ìˆìœ¼ë©´ ì†ŒìŠ¤ í•„í„°ë§ ì¶”ê°€
    query = keyword
    if source:
        query = f"{keyword} site:{get_source_domain(source)}"
    
    # URL ì¸ì½”ë”©
    query = requests.utils.quote(query)
    url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
    debug_print(f"Google News RSS ìš”ì²­ URL: {url}")
    
    try:
        feed = feedparser.parse(url)
        
        articles = []
        for entry in feed.entries[:15]:  # ì¡°ê¸ˆ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§í•  ì—¬ì§€ í™•ë³´
            # ì†ŒìŠ¤ ì¶”ì¶œ
            title_parts = entry.title.split(" - ")
            entry_source = title_parts[-1].strip() if len(title_parts) > 1 else "Unknown"
            
            # ì†ŒìŠ¤ í™•ì¸ - ë” ì—„ê²©í•œ ê²€ì‚¬
            if source and not is_same_source(entry_source, source):
                debug_print(f"ì†ŒìŠ¤ ë¶ˆì¼ì¹˜: '{entry_source}' â‰  '{source}' - ê±´ë„ˆëœ€")
                continue
                
            article = {
                'title': title_parts[0].strip(),
                'content': entry.description if hasattr(entry, 'description') else title_parts[0].strip(),
                'source': entry_source,
                'url': entry.link,
                'published_at': entry.published if hasattr(entry, 'published') else datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            articles.append(article)
        
        debug_print(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼: {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
        return articles
        
    except Exception as e:
        debug_print(f"Google News RSS ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

# í‚¤ì›Œë“œ í™•ì¥ + Google News ê²€ìƒ‰ ê²°í•©
def search_expanded_news(keyword, source=None):
    """í‚¤ì›Œë“œ í™•ì¥ í›„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    debug_print(f"'{keyword}' í™•ì¥ ê²€ìƒ‰ ì‹œì‘ (ì†ŒìŠ¤: {source})", important=True)
    
    # Step 1: ì›ë˜ í‚¤ì›Œë“œë¡œ Google News ê²€ìƒ‰
    initial_news = get_google_news_rss(keyword, source)
    
    if len(initial_news) >= 5:  # ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        debug_print(f"ì›ë˜ í‚¤ì›Œë“œë¡œ ì¶©ë¶„í•œ ê²°ê³¼({len(initial_news)}ê°œ)ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return initial_news[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
    
    # Step 2: í‚¤ì›Œë“œ í™•ì¥ì´ í•„ìš”í•œ ê²½ìš°
    debug_print(f"ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ í™•ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    expanded_keywords = [keyword]  # ì›ë˜ í‚¤ì›Œë“œ í¬í•¨
    
    # ë‹¨ì¼ ë‹¨ì–´ì¸ ê²½ìš° WordNet ë™ì˜ì–´ ì¶”ê°€
    if ' ' not in keyword and len(keyword) > 3:
        synonyms = get_synonyms(keyword)[:3]  # ìƒìœ„ 3ê°œ ë™ì˜ì–´ë§Œ ì‚¬ìš©
        expanded_keywords.extend(synonyms)
    
    # ì¤‘ë³µ ì œê±°
    expanded_keywords = list(set(expanded_keywords))
    debug_print(f"í™•ì¥ëœ í‚¤ì›Œë“œ: {expanded_keywords}")
    
    # Step 3: í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
    all_news = initial_news.copy()  # ì´ˆê¸° ê²°ê³¼ í¬í•¨
    
    for k in expanded_keywords:
        if k == keyword:  # ì›ë˜ í‚¤ì›Œë“œëŠ” ì´ë¯¸ ê²€ìƒ‰í–ˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœ€
            continue
            
        debug_print(f"í™•ì¥ í‚¤ì›Œë“œ '{k}'ë¡œ ê²€ìƒ‰ ì¤‘...")
        additional_news = get_google_news_rss(k, source)
        
        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
        for article in additional_news:
            # URLë¡œ ì¤‘ë³µ í™•ì¸
            if not any(existing['url'] == article['url'] for existing in all_news):
                all_news.append(article)
    
    debug_print(f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(all_news)}ê°œ ê¸°ì‚¬")
    return all_news[:5]  # ìµœëŒ€ 5ê°œë§Œ ë°˜í™˜

# ê°ì • ë¶„ì„ í•¨ìˆ˜ë“¤

def get_vader_sentiment(text):
    """VADERë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        sid = SentimentIntensityAnalyzer()
        sentiment_scores = sid.polarity_scores(text)
        # compound ì ìˆ˜ëŠ” -1ì—ì„œ 1 ì‚¬ì´ì˜ ê°’
        compound_score = sentiment_scores['compound']
        print(f"VADER ì›ë³¸ ì ìˆ˜: {compound_score}")
        return compound_score
    except Exception as e:
        print(f"VADER ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0

def get_sentistrength_sentiment(text):
    """SentiStrengthë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        # SentiStrength ì´ˆê¸°í™”
        senti = PySentiStr()
        
        # SentiStrength.jar íŒŒì¼ ê²½ë¡œ ì„¤ì •
        senti.setSentiStrengthPath("C:\\Users\\user\\Desktop\\ì—°êµ¬\\êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬\\ì½”ë”©\\SentiStrength.jar")
        
        # ì–¸ì–´ í´ë” ê²½ë¡œ ì„¤ì •
        senti.setSentiStrengthLanguageFolderPath("C:\\Users\\user\\Desktop\\ì—°êµ¬\\êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬\\ì½”ë”©\\SentiStrengthDataEnglishOctober2019")
        
        # dual ì ìˆ˜ë¡œ ë¶„ì„ (ê¸ì • ë° ë¶€ì • ì ìˆ˜ ê°ê° ë°˜í™˜)
        result = senti.getSentiment(text, score='dual')
        
        # ê²°ê³¼ëŠ” íŠœí”Œ í˜•íƒœ: (positive_score, negative_score)
        positive_score, negative_score = result[0]
        
        print(f"SentiStrength ì›ë³¸ ì ìˆ˜: ê¸ì •={positive_score}, ë¶€ì •={negative_score}")
        return positive_score, negative_score
    except Exception as e:
        print(f"SentiStrength ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í”¼ë“œë°± ì¶”ê°€
        print(f"ê²½ë¡œ í™•ì¸: JAR={senti.SentiStrengthPath}, ë°ì´í„°={senti.SentiStrengthLanguageFolderPath}")
        print("Javaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: 'java -version' ëª…ë ¹ì–´ë¡œ í™•ì¸ ê°€ëŠ¥")
        return 1, -1  # ì¤‘ë¦½ ê°’ ë°˜í™˜

def get_google_sentiment(text):
    """Google Natural Language APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = GOOGLE_APPLICATION_CREDENTIALS
        
        # ì¸ì¦ íŒŒì¼ì´ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆëœ€
        if not GOOGLE_APPLICATION_CREDENTIALS:
            print("Google API ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = language_v1.LanguageServiceClient()
        
        # ë¬¸ì„œ ê°ì²´ ìƒì„±
        document = language_v1.Document(
            content=text,
            type_=language_v1.Document.Type.PLAIN_TEXT
        )
        
        # ê°ì • ë¶„ì„ ìˆ˜í–‰
        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment
        
        # Google APIëŠ” -1.0 ~ 1.0 ë²”ìœ„ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜
        score = sentiment.score
        print(f"Google API ì›ë³¸ ì ìˆ˜: {score}")
        return score
    except Exception as e:
        print(f"Google ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ê°’ ë°˜í™˜

def get_huggingface_sentiment(text):
    try:
        # ëª¨ë¸ ëª…ì‹œì  ì§€ì •ìœ¼ë¡œ ê²½ê³  ì œê±°
        sentiment_analyzer = pipeline("sentiment-analysis", 
                                    model="distilbert-base-uncased-finetuned-sst-2-english")
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë¶„ì„)
        max_length = 512
        truncated_text = text[:max_length] if len(text) > max_length else text
        result = sentiment_analyzer(truncated_text)[0]
        
        # NEGATIVE ê²°ê³¼ì— ëŒ€í•œ ì²˜ë¦¬ ê°œì„ 
        if result['label'] == 'POSITIVE':
            score = result['score']
        else:
            score = 1 - result['score']  # ë” ì§ê´€ì ì¸ ë³€í™˜
            
        print(f"Hugging Face ì ìˆ˜: {score} (ì›ë˜ ë ˆì´ë¸”: {result['label']})")
        return score
    except Exception as e:
        print(f"Hugging Face ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0.5

# ì ìˆ˜ ì •ê·œí™” í•¨ìˆ˜ë“¤

def normalize_vader_score(score):
    """VADER ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    normalized_score = (score + 1) / 2
    return normalized_score

def normalize_sentistrength_score(positive_score, negative_score):
    """SentiStrength ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    combined_score = (positive_score + (6 + negative_score)) / 10
    return combined_score

def normalize_google_score(score):
    """Google API ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    normalized_score = (score + 1) / 2
    return normalized_score

# ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜

def calculate_final_sentiment(vader_score, google_score, huggingface_score, df=None):
    """ê° ë„êµ¬ì˜ ì •ê·œí™”ëœ ê°ì • ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚° (Hugging Face ì¡°ì • í¬í•¨)"""
    
    # Z-scoreë¥¼ í™œìš©í•˜ì—¬ Hugging Face ì ìˆ˜ê°€ ê·¹ë‹¨ì ì¸ì§€ íŒë‹¨
    if df is not None:
        df["huggingface_score_zscore"] = zscore(df["huggingface_score"])
        huggingface_zscore = df["huggingface_score_zscore"].iloc[-1]  # ë§ˆì§€ë§‰ ë°ì´í„° ê¸°ì¤€
    
        # Z-scoreê°€ Â±2.0 ì´ìƒì´ë©´ ê°€ì¤‘ì¹˜ ë‚®ì¶¤
        if abs(huggingface_zscore) > 2.0:
            huggingface_weight = 0.1  # ê¸°ì¡´ 0.2 â†’ 0.1ë¡œ ì¶•ì†Œ
            print(f"Hugging Face ê°ì • ì ìˆ˜ê°€ ê·¹ë‹¨ì ìœ¼ë¡œ í‰ê°€ë¨ (Z-score: {huggingface_zscore:.2f}) â†’ ê°€ì¤‘ì¹˜ ì¡°ì •")
        else:
            huggingface_weight = 0.2  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ìœ ì§€
    else:
        huggingface_weight = 0.2  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ìœ ì§€

    # ë‹¤ë¥¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    vader_weight = 0.4
    google_weight = 0.4

    # ì •ê·œí™”ëœ ê°ì • ì ìˆ˜ ê³„ì‚°
    normalized_vader = normalize_vader_score(vader_score)
    normalized_google = normalize_google_score(google_score)

    # ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚°
    final_score = (vader_weight * normalized_vader + 
                   google_weight * normalized_google + 
                   huggingface_weight * huggingface_score)
    
    return final_score

# ê¸°ì‚¬ ê°ì • ë¶„ì„ í•¨ìˆ˜

def calculate_z_scores(df):
    """ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ë¥¼ Z-scoreë¡œ ë³€í™˜"""
    z_score_df = df.copy()
    
    # ê°ì • ì ìˆ˜ Z-score ì ìš©
    z_score_df["final_sentiment_zscore"] = zscore(df["final_sentiment_score"])
    z_score_df["sentiment_intensity_zscore"] = zscore(df["sentiment_intensity_score"])
    
    # ê°ì • ë¶„ì„ ë„êµ¬ë³„ Z-score ì ìš©
    tools = ["vader_score", "google_score", "huggingface_score"]
    for tool in tools:
        z_score_df[f"{tool}_zscore"] = zscore(df[tool])

    return z_score_df

def analyze_article_sentiment(article, country, search_keyword=None, df=None):
    """ê¸°ì‚¬ ê°ì • ë¶„ì„ ìˆ˜í–‰ ë° Hugging Face ì ìˆ˜ ì¡°ì • í¬í•¨"""
    title = article["title"]
    content = article["content"]
    source = article["source"]
    text = f"{title}. {content}"

    print(f"\nì…ë ¥ ë‰´ìŠ¤ ê¸°ì‚¬: '{title}'")
    print(f"ì¶œì²˜: {source} (êµ­ê°€: {country})")
    print(f"ë‚´ìš© ì¼ë¶€: {content[:100]}...\n")

    # ê°ì • ë¶„ì„ ë„êµ¬ ì ìš©
    vader_score = get_vader_sentiment(text)
    google_score = get_google_sentiment(text)
    huggingface_score = get_huggingface_sentiment(text)
    sentistrength_pos, sentistrength_neg = get_sentistrength_sentiment(text)

    # ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ ê³„ì‚° (Hugging Face ê°€ì¤‘ì¹˜ ì¡°ì • í¬í•¨)
    final_sentiment_score = calculate_final_sentiment(vader_score, google_score, huggingface_score, df)
    sentiment_intensity_score = normalize_sentistrength_score(sentistrength_pos, sentistrength_neg)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n===== ê°ì • ë¶„ì„ ê²°ê³¼ =====")
    print(f"VADER ê°ì • ì ìˆ˜: {vader_score} (ì •ê·œí™”: {normalize_vader_score(vader_score):.4f})")
    print(f"Google API ê°ì • ì ìˆ˜: {google_score} (ì •ê·œí™”: {normalize_google_score(google_score):.4f})")
    print(f"Hugging Face ê°ì • ì ìˆ˜: {huggingface_score:.4f}")
    print(f"ìµœì¢… ê°ì • ì ìˆ˜: {final_sentiment_score:.4f}")
    print(f"SentiStrength ê°ì • ê°•ë„: ê¸ì •={sentistrength_pos}, ë¶€ì •={sentistrength_neg} (ì •ê·œí™”: {sentiment_intensity_score:.4f})")

    return {
        "title": title,
        "source": source,
        "country": country,
        "bias": get_source_bias(source),
        "search_keyword": search_keyword,
        "final_sentiment_score": final_sentiment_score,
        "sentiment_intensity_score": sentiment_intensity_score,
        "vader_score": normalize_vader_score(vader_score),
        "google_score": normalize_google_score(google_score),
        "huggingface_score": huggingface_score,
        "url": article.get("url", ""),
    }

def get_source_bias(source):
    """ì‹ ë¬¸ì‚¬ì˜ ì •ì¹˜ì  ì„±í–¥ ë°˜í™˜"""
    for country, sources in NEWS_SOURCES.items():
        if source in sources:
            return sources[source]['bias']
    return "unknown"

# êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ í•¨ìˆ˜

def compare_news_sources(keywords, articles_per_source=3):
    """ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œë³„ ê°ì • ë¶„ì„ ë¹„êµ"""
    results = []
    
    # ì›í•˜ëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤ë¡œ ì •í™•íˆ ì œí•œ
    sources = {
        'CNN': 'US',
        'Fox News': 'US',
        'The Guardian': 'UK',
        'The New York Times': 'US',
        'The Telegraph': 'UK',
        'BBC News': 'UK'
    }
    
    for keyword in keywords:
        debug_print(f"\ní‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ë¶„ì„ ì‹œì‘...", important=True)
        
        for source, country in sources.items():
            debug_print(f"{source}({country})ì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
            
            # Google News RSSë¡œ ê¸°ì‚¬ ê²€ìƒ‰
            articles = search_expanded_news(keyword, source)
            
            if not articles:
                debug_print(f"âŒ {source}ì—ì„œ '{keyword}'ì— ëŒ€í•œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ì°¾ì€ ê¸°ì‚¬ ê°œìˆ˜ ì œí•œ
            articles = articles[:articles_per_source]
            
            # ê° ê¸°ì‚¬ ë¶„ì„
            for article in articles:
                # ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                time.sleep(randint(1, 3))
                
                result = analyze_article_sentiment(article, country, search_keyword=keyword)
                
                if result:
                    results.append(result)
                    debug_print(f"âœ… ë¶„ì„ ì™„ë£Œ: {result['title']} (ì ìˆ˜: {result['final_sentiment_score']})")
    
    return results

# ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥ í•¨ìˆ˜

def get_date_folder():
    """ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í´ë”ëª… ìƒì„± (ì˜ˆ: 20240601)"""
    today = datetime.datetime.now()
    return today.strftime("%Y%m%d")

def get_timestamp():
    """í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ì˜ˆ: 20240601_143045)"""
    now = datetime.datetime.now()
    return now.strftime("%Y%m%d_%H%M%S")

def get_result_folder():
    """ë‚ ì§œë³„ ê²°ê³¼ í´ë” ê²½ë¡œ ë°˜í™˜ ë° ìƒì„±"""
    date_folder = get_date_folder()
    result_path = os.path.join(RESULTS_DIR, date_folder)
    
    # ê²°ê³¼ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)
    
    # ë‚ ì§œë³„ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(result_path):
        os.makedirs(result_path)
        print(f"ìƒˆ ë‚ ì§œ í´ë” ìƒì„±: {result_path}")
    
    return result_path

def sample_article_review(df, sample_size=5):
    """Sample article review for sentiment analysis validation"""
    print("\nğŸ” Sample Article Sentiment Analysis Review")
    if len(df) < sample_size:
        sample_size = len(df)
        print(f"âš ï¸ Only {sample_size} samples available for review.")
    
    sample_articles = df.sample(sample_size)
    for idx, row in sample_articles.iterrows():
        print(f"\nArticle Title: {row['title']}")
        print(f"Source: {row['source']} (Country: {row['country']})")
        print(f"Search Keyword: {row.get('search_keyword', 'No info')}")
        print(f"Final Sentiment Score: {row['final_sentiment_score']:.4f} ({row['sentiment']})")
        print(f"Tool Scores: VADER={row['vader_score']:.2f}, SentiStrength={row['sentistrength_score']:.2f}, " +
              f"Google={row['google_score']:.2f}, HuggingFace={row['huggingface_score']:.2f}")
        print(f"URL: {row.get('url', 'No info')}")
        print("=" * 50)

def visualize_sentiment_by_source(df, result_folder, timestamp):
    """Compare sentiment score distribution by news source"""
    plt.figure(figsize=(12, 6))
    ax = sns.boxplot(x='source', y='final_sentiment_score', data=df)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    plt.title('Sentiment Score Distribution by News Source')
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'source_sentiment_boxplot_{timestamp}.png'))
    print(f"ğŸ“Š News source sentiment distribution graph saved.")

def compare_sentiment_tools(df, result_folder, timestamp):
    """Compare sentiment scores across different analysis tools"""
    plt.figure(figsize=(10, 6))
    tools = ['vader_score', 'sentistrength_score', 'google_score', 'huggingface_score']
    tool_labels = ['VADER', 'SentiStrength', 'Google NLP', 'HuggingFace']
    
    # Box plot
    ax = sns.boxplot(data=df[tools])
    ax.set_xticklabels(tool_labels)
    plt.title('Sentiment Score Comparison Across Analysis Tools')
    plt.ylabel('Sentiment Score (0-1)')
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'tool_comparison_{timestamp}.png'))
    
    # Correlation analysis
    plt.figure(figsize=(10, 8))
    correlation = df[tools].corr()
    sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, 
                xticklabels=tool_labels, yticklabels=tool_labels)
    plt.title('Correlation Between Sentiment Analysis Tools')
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'tool_correlation_{timestamp}.png'))
    print(f"ğŸ“Š Tool comparison graphs saved.")

def analyze_bias_influence(df, result_folder, timestamp):
    """Analyze the influence of political bias on sentiment analysis"""
    plt.figure(figsize=(12, 6))
    
    # Political bias sentiment distribution
    sns.boxplot(x='bias', y='final_sentiment_score', data=df)
    plt.title('Sentiment Score Distribution by Political Bias')
    plt.xlabel('Political Bias')
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'bias_influence_{timestamp}.png'))
    
    # Tool-specific bias analysis
    tools = ['vader_score', 'sentistrength_score', 'google_score', 'huggingface_score']
    tool_labels = ['VADER', 'SentiStrength', 'Google NLP', 'HuggingFace']
    
    plt.figure(figsize=(15, 10))
    for i, tool in enumerate(tools):
        plt.subplot(2, 2, i+1)
        sns.boxplot(x='bias', y=tool, data=df)
        plt.title(f'{tool_labels[i]}')
        plt.xlabel('Political Bias')
        plt.ylabel('Sentiment Score (0-1)')
        plt.ylim(0, 1)
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'bias_by_tool_{timestamp}.png'))
    print(f"ğŸ“Š Political bias influence graphs saved.")

def visualize_sentiment_intensity(df, result_folder, timestamp):
    """ê°ì • ê°•ë„(SentiStrength)ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°í™”"""
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='bias', y='sentiment_intensity_score', data=df)
    plt.title('Sentiment Intensity by Political Bias')
    plt.ylabel('Sentiment Intensity Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'sentiment_intensity_{timestamp}.png'))
    print(f"ğŸ“Š Sentiment intensity graph saved.")
    
    # ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ì‹œê°í™”ë„ ì¶”ê°€
    plt.figure(figsize=(12, 6))
    ax = sns.boxplot(x='source', y='sentiment_intensity_score', data=df)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    plt.title('Sentiment Intensity Distribution by News Source')
    plt.ylabel('Sentiment Intensity Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'source_intensity_boxplot_{timestamp}.png'))
    print(f"ğŸ“Š News source intensity distribution graph saved.")

def evaluate_sentiment_reliability(df, result_folder, timestamp):
    """Comprehensive evaluation of sentiment analysis reliability"""
    print("\n=== Starting Sentiment Analysis Reliability Evaluation ===")
    
    # 1. Sample article review
    sample_article_review(df)
    
    # 2. News source sentiment comparison
    visualize_sentiment_by_source(df, result_folder, timestamp)
    
    # 3. Analysis tool comparison
    compare_sentiment_tools(df, result_folder, timestamp)
    
    # 4. Political bias influence analysis
    analyze_bias_influence(df, result_folder, timestamp)
    
    # 5. Sentiment intensity analysis (ìƒˆë¡œ ì¶”ê°€)
    visualize_sentiment_intensity(df, result_folder, timestamp)
    
    print("\n=== Sentiment Analysis Reliability Evaluation Complete ===")
    print(f"All evaluation graphs have been saved to {result_folder} folder.")

def calculate_statistics(df):
    """ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ì˜ í‰ê· , ë¶„ì‚°, í‘œì¤€í¸ì°¨ ê³„ì‚°"""
    stats = {}

    # ì „ì²´ ê¸°ì‚¬ì— ëŒ€í•œ ì¢…í•© í†µê³„
    stats["ì „ì²´_ê°ì •ì ìˆ˜_í‰ê· "] = df["final_sentiment_score"].mean()
    stats["ì „ì²´_ê°ì •ì ìˆ˜_ë¶„ì‚°"] = df["final_sentiment_score"].var()
    stats["ì „ì²´_ê°ì •ì ìˆ˜_í‘œì¤€í¸ì°¨"] = df["final_sentiment_score"].std()

    stats["ì „ì²´_ê°ì •ê°•ë„_í‰ê· "] = df["sentiment_intensity_score"].mean()
    stats["ì „ì²´_ê°ì •ê°•ë„_ë¶„ì‚°"] = df["sentiment_intensity_score"].var()
    stats["ì „ì²´_ê°ì •ê°•ë„_í‘œì¤€í¸ì°¨"] = df["sentiment_intensity_score"].std()

    # êµ­ê°€ë³„ í†µê³„
    country_stats = df.groupby("country").agg(
        ê°ì •ì ìˆ˜_í‰ê· =("final_sentiment_score", "mean"),
        ê°ì •ì ìˆ˜_ë¶„ì‚°=("final_sentiment_score", "var"),
        ê°ì •ì ìˆ˜_í‘œì¤€í¸ì°¨=("final_sentiment_score", "std"),
        ê°ì •ê°•ë„_í‰ê· =("sentiment_intensity_score", "mean"),
        ê°ì •ê°•ë„_ë¶„ì‚°=("sentiment_intensity_score", "var"),
        ê°ì •ê°•ë„_í‘œì¤€í¸ì°¨=("sentiment_intensity_score", "std")
    )

    # ì •ì¹˜ ì„±í–¥ë³„ í†µê³„
    bias_stats = df.groupby("bias").agg(
        ê°ì •ì ìˆ˜_í‰ê· =("final_sentiment_score", "mean"),
        ê°ì •ì ìˆ˜_ë¶„ì‚°=("final_sentiment_score", "var"),
        ê°ì •ì ìˆ˜_í‘œì¤€í¸ì°¨=("final_sentiment_score", "std"),
        ê°ì •ê°•ë„_í‰ê· =("sentiment_intensity_score", "mean"),
        ê°ì •ê°•ë„_ë¶„ì‚°=("sentiment_intensity_score", "var"),
        ê°ì •ê°•ë„_í‘œì¤€í¸ì°¨=("sentiment_intensity_score", "std")
    )

    return stats, country_stats, bias_stats

def save_statistics_to_csv(df, result_folder, timestamp):
    """ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ì˜ ì¢…í•© í†µê³„ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    stats, country_stats, bias_stats = calculate_statistics(df)

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    filepath = os.path.join(result_folder, f"sentiment_statistics_{timestamp}.csv")

    # ë°ì´í„° ì •ë¦¬
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # ì „ì²´ ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ í†µê³„
        writer.writerow(["ì¢…í•© í†µê³„"])
        writer.writerow(["í•­ëª©", "ê°’"])
        for key, value in stats.items():
            writer.writerow([key, value])

        writer.writerow([])  # ë¹ˆ ì¤„ ì¶”ê°€
        
        # êµ­ê°€ë³„ í†µê³„ ì €ì¥
        writer.writerow(["êµ­ê°€ë³„ ê°ì • í†µê³„"])
        writer.writerow(["êµ­ê°€", "ê°ì •ì ìˆ˜_í‰ê· ", "ê°ì •ì ìˆ˜_ë¶„ì‚°", "ê°ì •ì ìˆ˜_í‘œì¤€í¸ì°¨", 
                         "ê°ì •ê°•ë„_í‰ê· ", "ê°ì •ê°•ë„_ë¶„ì‚°", "ê°ì •ê°•ë„_í‘œì¤€í¸ì°¨"])
        for country, row in country_stats.iterrows():
            writer.writerow([country] + row.tolist())

        writer.writerow([])  # ë¹ˆ ì¤„ ì¶”ê°€
        
        # ì •ì¹˜ ì„±í–¥ë³„ í†µê³„ ì €ì¥
        writer.writerow(["ì •ì¹˜ ì„±í–¥ë³„ ê°ì • í†µê³„"])
        writer.writerow(["ì„±í–¥", "ê°ì •ì ìˆ˜_í‰ê· ", "ê°ì •ì ìˆ˜_ë¶„ì‚°", "ê°ì •ì ìˆ˜_í‘œì¤€í¸ì°¨", 
                         "ê°ì •ê°•ë„_í‰ê· ", "ê°ì •ê°•ë„_ë¶„ì‚°", "ê°ì •ê°•ë„_í‘œì¤€í¸ì°¨"])
        for bias, row in bias_stats.iterrows():
            writer.writerow([bias] + row.tolist())

    print(f"\nğŸ“Š ê°ì • ë¶„ì„ ì¢…í•© í†µê³„ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def visualize_results(results):
    """Z-scoreë¥¼ í¬í•¨í•œ ê°ì • ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
    if not results:
        print("No results to visualize.")
        return

    result_folder = get_result_folder()
    timestamp = get_timestamp()

    df = pd.DataFrame(results)
    df = calculate_z_scores(df)  # Z-score ë³€í™˜ ì ìš©
    
    # ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ í†µê³„ ì €ì¥
    save_statistics_to_csv(df, result_folder, timestamp)

    # Add analysis time to title
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    title_suffix = f" (Analysis: {current_time})"
    
    # 1. êµ­ê°€ë³„ ê°ì • ì ìˆ˜ Z-score ë¹„êµ
    plt.figure(figsize=(10, 6))
    country_avg = df.groupby("country")["final_sentiment_zscore"].mean().sort_values(ascending=False)
    country_avg.plot(kind="bar", color="skyblue")
    plt.title("Average Sentiment Z-score by Country" + title_suffix)
    plt.ylabel("Sentiment Z-score")
    plt.axhline(y=0, color="r", linestyle="-", alpha=0.3)  # í‰ê· ì„ 
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f"country_sentiment_zscore_{timestamp}.png"))
    
    # 2. ì‹ ë¬¸ì‚¬ë³„ ê°ì • ì ìˆ˜ Z-score ë¹„êµ
    plt.figure(figsize=(12, 6))
    source_avg = df.groupby("source")["final_sentiment_zscore"].mean().sort_values(ascending=False)
    source_avg.plot(kind="bar", color="lightgreen")
    plt.title("Average Sentiment Z-score by News Source" + title_suffix)
    plt.ylabel("Sentiment Z-score")
    plt.axhline(y=0, color="r", linestyle="-", alpha=0.3)  # í‰ê· ì„ 
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f"source_sentiment_zscore_{timestamp}.png"))
    
    # 3. ì •ì¹˜ì  ì„±í–¥ë³„ ê°ì • ì ìˆ˜ Z-score ë¹„êµ
    plt.figure(figsize=(8, 6))
    bias_avg = df.groupby('bias')['final_sentiment_zscore'].mean().sort_values(ascending=False)
    colors = {'liberal': 'blue', 'conservative': 'red', 'neutral': 'gray', 'unknown': 'black'}
    bias_avg.plot(kind='bar', color=[colors.get(x, 'black') for x in bias_avg.index])
    plt.title('Average Sentiment Z-score by Political Bias' + title_suffix)
    plt.ylabel('Sentiment Z-score')
    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'bias_sentiment_zscore_{timestamp}.png'))

    # 4. ê°ì • ë¶„ì„ ë„êµ¬ë³„ ê°ì • ì ìˆ˜ Z-score ë¹„êµ
    plt.figure(figsize=(10, 6))
    tools = ["vader_score_zscore", "google_score_zscore", "huggingface_score_zscore"]
    tool_labels = ["VADER", "Google NLP", "HuggingFace"]
    tool_avg = df[tools].mean()
    tool_avg.index = tool_labels  # ë ˆì´ë¸” ë³€ê²½
    tool_avg.plot(kind="bar", color="orange")
    plt.title("Average Sentiment Z-score by Analysis Tool" + title_suffix)
    plt.ylabel("Sentiment Z-score")
    plt.axhline(y=0, color="r", linestyle="-", alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f"tool_sentiment_zscore_{timestamp}.png"))

    print(f"\nğŸ“Š Z-score ê¸°ë°˜ ê°ì • ë¶„ì„ ê²°ê³¼ ì‹œê°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì‹ ë¢°ë„ í‰ê°€ ì‹¤í–‰
    evaluate_sentiment_reliability(df, result_folder, timestamp)

def save_results_to_csv(results, filename=None):
    """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ë©° Z-score í¬í•¨"""
    if not results:
        print("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    result_folder = get_result_folder()
    timestamp = get_timestamp()
    if filename is None:
        filename = f"news_sentiment_analysis_{timestamp}.csv"

    # ê²°ê³¼ì— ë¶„ì„ ì‹œê°„ ì¶”ê°€
    analysis_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    for r in results:
        r['analysis_time'] = analysis_time
        
    df = pd.DataFrame(results)
    df = calculate_z_scores(df)  # Z-score ì ìš©

    filepath = os.path.join(result_folder, filename)
    df.to_csv(filepath, index=False, encoding="utf-8")

    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì•Œë¦¼: êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµë¥¼ ìœ„í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ê²€ìƒ‰í•  í‚¤ì›Œë“œ ëª©ë¡
    keywords = [
        "W.T.O", "WTO", "World Trade Organization",
        "tariff", "tariffs",
        "trade war",
        "free trading", "free trade",
        "trade liberalization",
        "trade agreements", "trade deal",
        "global trade", "international trade",
        "import export regulations",
        "trade deficit", "trade surplus",
        "protectionism", "trade barriers"
    ]
    
    # í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œëŠ” í‚¤ì›Œë“œ ìˆ˜ ì œí•œ
    if DEBUG_MODE:
        test_keywords = ["trade"]
        debug_print(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {test_keywords} í‚¤ì›Œë“œë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", important=True)
        results = compare_news_sources(test_keywords, articles_per_source=1)
        
        if not results:
            debug_print("âŒ í…ŒìŠ¤íŠ¸ì—ì„œ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        else:
            debug_print(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼ ìƒì„±")
            
            # í…ŒìŠ¤íŠ¸ í›„ ê³„ì† ì§„í–‰í• ì§€ ë¬¼ì–´ë´„
            continue_analysis = input("í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì „ì²´ ë¶„ì„ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
            if not continue_analysis:
                return
    
    # ì „ì²´ í‚¤ì›Œë“œë¡œ ì‹¤í–‰
    results = compare_news_sources(keywords, articles_per_source=3)
    
    # ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
    if results:
        save_results_to_csv(results)
        visualize_results(results)
        debug_print(f"âœ… ì´ {len(results)}ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì €ì¥ë¨.")
    else:
        debug_print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()